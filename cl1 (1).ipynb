{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58TOf9Vb9arN",
        "outputId": "b108d402-55e1-4a6e-f78c-88b597f834cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cdsapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbab21GW9j6K",
        "outputId": "5a94cc12-118b-460c-d461-87ac53a52b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cdsapi\n",
            "  Downloading cdsapi-0.6.1.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from cdsapi) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cdsapi) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->cdsapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->cdsapi) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->cdsapi) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->cdsapi) (2024.2.2)\n",
            "Building wheels for collected packages: cdsapi\n",
            "  Building wheel for cdsapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdsapi: filename=cdsapi-0.6.1-py2.py3-none-any.whl size=12006 sha256=617a0447d698a3c5518b8baea985db414133cb2e06bceb724ee527767c16c484\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/63/08/45461d6f6636c1aba7846828d8c787a064073945048f76d44a\n",
            "Successfully built cdsapi\n",
            "Installing collected packages: cdsapi\n",
            "Successfully installed cdsapi-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cdsKey = \"294342:19fc170f-4d11-469c-a561-1efc7bcad06f\""
      ],
      "metadata": {
        "id": "yhSRqV8UCcsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cdsapi"
      ],
      "metadata": {
        "id": "699lkfI7-UFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_copernicus_era5(dst, variable, year, regions={\"Globe\": [90, -180, -90, 180]}, pressure=False, api_key=None):\n",
        "    if api_key is not None:\n",
        "        content = f\"url: https://cds.climate.copernicus.eu/api/v2\\nkey: {api_key}\"\n",
        "        home_dir = os.environ[\"HOME\"]\n",
        "        with open(os.path.join(home_dir, \".cdsapirc\"), \"w\") as f:\n",
        "            f.write(content)\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    client = cdsapi.Client()\n",
        "    download_args = {\n",
        "        \"product_type\": \"reanalysis\",\n",
        "        \"format\": \"netcdf\",\n",
        "        \"variable\": variable,\n",
        "        \"year\": str(year),\n",
        "        \"month\": \"03\", #[str(i).rjust(2, \"0\") for i in range(1, 13)],\n",
        "        \"day\": \"01\", #[str(i).rjust(2, \"0\") for i in range(1, 32)],\n",
        "        \"time\": [str(i).rjust(2, \"0\") + \":00\" for i in range(0, 24)],\n",
        "    }\n",
        "    if pressure:\n",
        "        src = \"reanalysis-era5-pressure-levels\"\n",
        "        download_args[\"pressure_level\"] = [1000, 850, 500, 50]\n",
        "    else:\n",
        "        src = \"reanalysis-era5-single-levels\"\n",
        "\n",
        "    for regName, area in regions.items():\n",
        "      download_args[\"area\"] = area\n",
        "      client.retrieve(src, download_args, f\"{dst}/{variable}_{year}_{regName}_0.25deg.nc\")"
      ],
      "metadata": {
        "id": "UcQyqACN-fok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twoRegs = {\n",
        "    \"CA\": [50, -126, 30, -112],\n",
        "    \"Globe\": [90, -180, -90, 180],\n",
        "}"
      ],
      "metadata": {
        "id": "AorsS4XxtGPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oneReg = {\n",
        "    \"CA\": [50, -126, 30, -112]\n",
        "}"
      ],
      "metadata": {
        "id": "wg0XYysC8Tu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables = ['2m_temperature', 'total_precipitation']\n",
        "for var in variables:\n",
        "  download_copernicus_era5(\"data\", var, 2024, regions=oneReg, api_key=cdsKey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t1JxvP0MlTe",
        "outputId": "f4d75f6e-31f1-4a06-d58a-24f4278d8c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-03 21:53:15,553 INFO Welcome to the CDS\n",
            "INFO:cdsapi:Welcome to the CDS\n",
            "2024-04-03 21:53:15,558 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
            "INFO:cdsapi:Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
            "2024-04-03 21:53:15,857 INFO Request is queued\n",
            "INFO:cdsapi:Request is queued\n",
            "2024-04-03 21:53:18,601 INFO Request is running\n",
            "INFO:cdsapi:Request is running\n",
            "2024-04-03 21:53:24,475 INFO Request is completed\n",
            "INFO:cdsapi:Request is completed\n",
            "2024-04-03 21:53:24,480 INFO Downloading https://download-0018.copernicus-climate.eu/cache-compute-0018/cache/data6/adaptor.mars.internal-1712181201.8340993-28264-19-7554ada1-2694-4c89-adb1-0c6932a16013.nc to data/2m_temperature_2024_CA_0.25deg.nc (218.1K)\n",
            "INFO:cdsapi:Downloading https://download-0018.copernicus-climate.eu/cache-compute-0018/cache/data6/adaptor.mars.internal-1712181201.8340993-28264-19-7554ada1-2694-4c89-adb1-0c6932a16013.nc to data/2m_temperature_2024_CA_0.25deg.nc (218.1K)\n",
            "2024-04-03 21:53:25,741 INFO Download rate 173.5K/s\n",
            "INFO:cdsapi:Download rate 173.5K/s\n",
            "2024-04-03 21:53:25,985 INFO Welcome to the CDS\n",
            "INFO:cdsapi:Welcome to the CDS\n",
            "2024-04-03 21:53:25,989 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
            "INFO:cdsapi:Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
            "2024-04-03 21:53:26,219 INFO Request is queued\n",
            "INFO:cdsapi:Request is queued\n",
            "2024-04-03 21:53:28,965 INFO Request is running\n",
            "INFO:cdsapi:Request is running\n",
            "2024-04-03 21:53:31,340 INFO Request is completed\n",
            "INFO:cdsapi:Request is completed\n",
            "2024-04-03 21:53:31,345 INFO Downloading https://download-0012-clone.copernicus-climate.eu/cache-compute-0012/cache/data4/adaptor.mars.internal-1712181210.4086318-3889-8-278b0e20-5006-4977-9ef6-7cffa7e0c2db.nc to data/total_precipitation_2024_CA_0.25deg.nc (218.1K)\n",
            "INFO:cdsapi:Downloading https://download-0012-clone.copernicus-climate.eu/cache-compute-0012/cache/data4/adaptor.mars.internal-1712181210.4086318-3889-8-278b0e20-5006-4977-9ef6-7cffa7e0c2db.nc to data/total_precipitation_2024_CA_0.25deg.nc (218.1K)\n",
            "2024-04-03 21:53:32,598 INFO Download rate 174.4K/s\n",
            "INFO:cdsapi:Download rate 174.4K/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import glob\n",
        "\n",
        "# Third party\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "OceYawPzAS9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NAME_TO_VAR = {\n",
        "    \"2m_temperature\": \"t2m\",\n",
        "    \"10m_u_component_of_wind\": \"u10\",\n",
        "    \"10m_v_component_of_wind\": \"v10\",\n",
        "    \"mean_sea_level_pressure\": \"msl\",\n",
        "    \"surface_pressure\": \"sp\",\n",
        "    \"toa_incident_solar_radiation\": \"tisr\",\n",
        "    \"total_precipitation\": \"tp\",\n",
        "    \"land_sea_mask\": \"lsm\",\n",
        "    \"orography\": \"orography\",\n",
        "    \"lattitude\": \"lat2d\",\n",
        "    \"geopotential\": \"z\",\n",
        "    \"u_component_of_wind\": \"u\",\n",
        "    \"v_component_of_wind\": \"v\",\n",
        "    \"temperature\": \"t\",\n",
        "    \"relative_humidity\": \"r\",\n",
        "    \"specific_humidity\": \"q\",\n",
        "    \"vorticity\": \"vo\",\n",
        "    \"potential_vorticity\": \"pv\",\n",
        "    \"total_cloud_cover\": \"tcc\",\n",
        "}"
      ],
      "metadata": {
        "id": "enYg1c92PtI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.arange(12)\n",
        "b = np.arange(11)\n",
        "print(a[:20])\n",
        "print(b[-20:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1TbyiPHREf5",
        "outputId": "055c9709-4e81-4dec-8aaa-d4069eee2d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9j_7_sVIbf",
        "outputId": "d49198a7-e582-41f1-8498-0ff6025fe54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m_temperature_2024_CA_0.25deg.nc  total_precipitation_2024_CA_0.25deg.nc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HOURS_PER_YEAR = 24  # look at the shard with small data\n",
        "\n",
        "def nc2np(path, variables, years, save_dir, partition, num_shards_per_year):\n",
        "    os.makedirs(os.path.join(save_dir, partition), exist_ok=True)\n",
        "\n",
        "    if partition == \"train\":\n",
        "        normalize_mean = {}\n",
        "        normalize_std = {}\n",
        "    climatology = {}\n",
        "\n",
        "    for year in tqdm(years):\n",
        "        np_vars = {}\n",
        "\n",
        "        # non-constant fields\n",
        "        for var in variables:\n",
        "            ps = glob.glob(os.path.join(path, f\"*{var}*{year}*.nc\"))\n",
        "            ds = xr.open_mfdataset(\n",
        "                ps, combine=\"by_coords\", parallel=True\n",
        "            )  # dataset for a single variable\n",
        "            code = NAME_TO_VAR[var]\n",
        "            print(ds[code].shape)\n",
        "            if len(ds[code].shape) == 3:  # surface level variables\n",
        "                ds[code] = ds[code].expand_dims(\"val\", axis=1)\n",
        "                # remove the last 24 hours if this year has 366 days\n",
        "                if code == \"tp\":  # accumulate 6 hours and log transform\n",
        "                    tp = ds[code].to_numpy()\n",
        "                    print(tp[:,0, 0,0])\n",
        "                    tp_cum_6hrs = np.cumsum(tp, axis=0)\n",
        "                    tp_cum_6hrs[6:] = tp_cum_6hrs[6:] - tp_cum_6hrs[:-6]\n",
        "                    print(tp_cum_6hrs[:, 0, 0, 0])\n",
        "                    eps = 0.001\n",
        "                    tp_cum_6hrs = np.log(eps + tp_cum_6hrs) - np.log(eps)\n",
        "                    np_vars[var] = tp_cum_6hrs[-HOURS_PER_YEAR:]\n",
        "                    print(np_vars[var][:, 0, 0, 0])\n",
        "                else:\n",
        "                    np_vars[var] = ds[code].to_numpy()[-HOURS_PER_YEAR:]\n",
        "\n",
        "                if partition == \"train\":\n",
        "                    # compute mean and std of each var in each year\n",
        "                    var_mean_yearly = np_vars[var].mean(axis=(0, 2, 3))\n",
        "                    var_std_yearly = np_vars[var].std(axis=(0, 2, 3))\n",
        "                    if var not in normalize_mean:\n",
        "                        normalize_mean[var] = [var_mean_yearly]\n",
        "                        normalize_std[var] = [var_std_yearly]\n",
        "                    else:\n",
        "                        normalize_mean[var].append(var_mean_yearly)\n",
        "                        normalize_std[var].append(var_std_yearly)\n",
        "\n",
        "                clim_yearly = np_vars[var].mean(axis=0)\n",
        "                if var not in climatology:\n",
        "                    climatology[var] = [clim_yearly]\n",
        "                else:\n",
        "                    climatology[var].append(clim_yearly)\n",
        "\n",
        "            else:  # pressure-level variables\n",
        "                assert len(ds[code].shape) == 4\n",
        "                all_levels = ds[\"level\"][:].to_numpy()\n",
        "                all_levels = np.intersect1d(all_levels, DEFAULT_PRESSURE_LEVELS)\n",
        "                for level in all_levels:\n",
        "                    ds_level = ds.sel(level=[level])\n",
        "                    level = int(level)\n",
        "                    # remove the last 24 hours if this year has 366 days\n",
        "                    np_vars[f\"{var}_{level}\"] = ds_level[code].to_numpy()[\n",
        "                        -HOURS_PER_YEAR:\n",
        "                    ]\n",
        "\n",
        "                    if partition == \"train\":\n",
        "                        # compute mean and std of each var in each year\n",
        "                        var_mean_yearly = np_vars[f\"{var}_{level}\"].mean(axis=(0, 2, 3))\n",
        "                        var_std_yearly = np_vars[f\"{var}_{level}\"].std(axis=(0, 2, 3))\n",
        "                        if f\"{var}_{level}\" not in normalize_mean:\n",
        "                            normalize_mean[f\"{var}_{level}\"] = [var_mean_yearly]\n",
        "                            normalize_std[f\"{var}_{level}\"] = [var_std_yearly]\n",
        "                        else:\n",
        "                            normalize_mean[f\"{var}_{level}\"].append(var_mean_yearly)\n",
        "                            normalize_std[f\"{var}_{level}\"].append(var_std_yearly)\n",
        "\n",
        "                    clim_yearly = np_vars[f\"{var}_{level}\"].mean(axis=0)\n",
        "                    if f\"{var}_{level}\" not in climatology:\n",
        "                        climatology[f\"{var}_{level}\"] = [clim_yearly]\n",
        "                    else:\n",
        "                        climatology[f\"{var}_{level}\"].append(clim_yearly)\n",
        "\n",
        "        assert HOURS_PER_YEAR % num_shards_per_year == 0\n",
        "        num_hrs_per_shard = HOURS_PER_YEAR // num_shards_per_year\n",
        "        for shard_id in range(num_shards_per_year):\n",
        "            start_id = shard_id * num_hrs_per_shard\n",
        "            end_id = start_id + num_hrs_per_shard\n",
        "            sharded_data = {k: np_vars[k][start_id:end_id] for k in np_vars.keys()}\n",
        "            np.savez(\n",
        "                os.path.join(save_dir, partition, f\"{year}_{shard_id}.npz\"),\n",
        "                **sharded_data,\n",
        "            )\n",
        "        print(\"===\")\n",
        "        for k in np_vars.keys():\n",
        "          print(k, np_vars[k].shape)\n",
        "        print(\"===\")\n",
        "\n",
        "    if partition == \"train\":\n",
        "        for var in normalize_mean.keys():\n",
        "            if not constants_are_downloaded or var not in constant_fields:\n",
        "                normalize_mean[var] = np.stack(normalize_mean[var], axis=0)\n",
        "                normalize_std[var] = np.stack(normalize_std[var], axis=0)\n",
        "\n",
        "        for var in normalize_mean.keys():  # aggregate over the years\n",
        "            if not constants_are_downloaded or var not in constant_fields:\n",
        "                mean, std = normalize_mean[var], normalize_std[var]\n",
        "                # var(X) = E[var(X|Y)] + var(E[X|Y])\n",
        "                variance = (\n",
        "                    (std**2).mean(axis=0)\n",
        "                    + (mean**2).mean(axis=0)\n",
        "                    - mean.mean(axis=0) ** 2\n",
        "                )\n",
        "                std = np.sqrt(variance)\n",
        "                # E[X] = E[E[X|Y]]\n",
        "                mean = mean.mean(axis=0)\n",
        "                normalize_mean[var] = mean\n",
        "                if var == \"total_precipitation\":\n",
        "                    normalize_mean[var] = np.zeros_like(normalize_mean[var])\n",
        "                normalize_std[var] = std\n",
        "\n",
        "        np.savez(os.path.join(save_dir, \"normalize_mean.npz\"), **normalize_mean)\n",
        "        np.savez(os.path.join(save_dir, \"normalize_std.npz\"), **normalize_std)\n",
        "\n",
        "    for var in climatology.keys():\n",
        "        climatology[var] = np.stack(climatology[var], axis=0)\n",
        "    climatology = {k: np.mean(v, axis=0) for k, v in climatology.items()}\n",
        "    print(\"Climatology !!!!!!\")\n",
        "    for k, v in climatology.items():\n",
        "      print(k, v.shape)\n",
        "    np.savez(\n",
        "        os.path.join(save_dir, partition, \"climatology.npz\"),\n",
        "        **climatology,\n",
        "    )"
      ],
      "metadata": {
        "id": "l-UlAe9kPGjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nc2np(\"data\", variables, [2024], \"processed\", \"test\", 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X08beLLyUOhw",
        "outputId": "f3b89228-3c08-4444-9e7b-a2aaa54fb920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24, 81, 57)\n",
            "(24, 81, 57)\n",
            "[2.6178826e-04 8.6783106e-04 2.1930784e-05 3.3346005e-05 1.7786934e-04\n",
            " 3.2709690e-04 4.1101594e-04 6.7328848e-04 7.2953431e-04 7.7630195e-04\n",
            " 9.5749216e-04 9.1743527e-04 6.4181024e-04 1.4016451e-04 4.7666952e-05\n",
            " 1.9094441e-05 8.5784122e-06 3.3346005e-05 7.5824326e-05 2.2221543e-04\n",
            " 3.7670112e-04 6.9805607e-04 1.7461781e-03 1.2502746e-03]\n",
            "[0.00026179 0.00112962 0.00115155 0.0011849  0.00136277 0.00168986\n",
            " 0.00183909 0.00164455 0.00235215 0.00309511 0.00387473 0.00446507\n",
            " 0.00469586 0.00416274 0.00348087 0.00272366 0.00177475 0.00089066\n",
            " 0.00032467 0.00040673 0.00073576 0.00141472 0.00315232 0.00436925]\n",
            "[0.23253012 0.7559433  0.7661886  0.7815685  0.85983276 0.98949003\n",
            " 1.0434837  0.9725003  1.2096024  1.4097929  1.584065   1.6983767\n",
            " 1.7397404  1.6414671  1.4998174  1.3147082  1.0205607  0.6369262\n",
            " 0.28116703 0.3412652  0.5514455  0.88158417 1.4236674  1.6806884 ]\n",
            "===\n",
            "2m_temperature (24, 1, 81, 57)\n",
            "total_precipitation (24, 1, 81, 57)\n",
            "===\n",
            "Climatology !!!!!!\n",
            "2m_temperature (1, 81, 57)\n",
            "total_precipitation (1, 81, 57)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset"
      ],
      "metadata": {
        "id": "zWvGwSTiYQwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NpyReader(IterableDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp_file_list,\n",
        "        out_file_list,\n",
        "        variables,\n",
        "        out_variables,\n",
        "        shuffle=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert len(inp_file_list) == len(out_file_list)\n",
        "        self.inp_file_list = [f for f in inp_file_list if \"climatology\" not in f]\n",
        "        self.out_file_list = [f for f in out_file_list if \"climatology\" not in f]\n",
        "        self.variables = variables\n",
        "        self.out_variables = out_variables if out_variables is not None else variables\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        n_files = len(self.inp_file_list)\n",
        "\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        if worker_info is None:\n",
        "            iter_start = 0\n",
        "            iter_end = n_files\n",
        "        else:\n",
        "            if not torch.distributed.is_initialized():\n",
        "                rank = 0\n",
        "                world_size = 1\n",
        "            else:\n",
        "                rank = torch.distributed.get_rank()\n",
        "                world_size = torch.distributed.get_world_size()\n",
        "            num_workers_per_ddp = worker_info.num_workers\n",
        "            num_shards = num_workers_per_ddp * world_size\n",
        "            per_worker = n_files // num_shards\n",
        "            worker_id = rank * num_workers_per_ddp + worker_info.id\n",
        "            iter_start = worker_id * per_worker\n",
        "            iter_end = iter_start + per_worker\n",
        "\n",
        "        for idx in range(iter_start, iter_end):\n",
        "            path_inp = self.inp_file_list[idx]\n",
        "            path_out = self.out_file_list[idx]\n",
        "            inp = np.load(path_inp)\n",
        "            if path_out == path_inp:\n",
        "                out = inp\n",
        "            else:\n",
        "                out = np.load(path_out)\n",
        "            yield {k: np.squeeze(inp[k], axis=1) for k in self.variables}, {\n",
        "                k: np.squeeze(out[k], axis=1) for k in self.out_variables\n",
        "            }, self.variables, self.out_variables"
      ],
      "metadata": {
        "id": "on5fPY6rUoKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_lister_test = sorted(\n",
        "    glob.glob(os.path.join(\"processed\", \"test\", \"*.npz\"))\n",
        ")\n",
        "out_lister_test = sorted(\n",
        "    glob.glob(os.path.join(\"processed\", \"test\", \"*.npz\"))\n",
        ")"
      ],
      "metadata": {
        "id": "TIIcAeTUYvwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inp_lister_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11qTIPdaV8n",
        "outputId": "fb2e587a-857f-41b2-e0c8-9138f77cdda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['processed/test/2024_0.npz', 'processed/test/2024_1.npz', 'processed/test/2024_2.npz', 'processed/test/2024_3.npz', 'processed/test/climatology.npz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = NpyReader(\n",
        "    inp_file_list=inp_lister_test,\n",
        "    out_file_list=out_lister_test,\n",
        "    variables=variables,\n",
        "    out_variables=variables,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "WP0EZORLaXvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for xIn, xOut, varIn, varOut in tmp:\n",
        "  for k, v in xIn.items():\n",
        "    print(k, v.shape)\n",
        "  print(varIn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piW8jEk3jAm3",
        "outputId": "3a83d9cb-1cab-46ff-a932-b3b70d1e6597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m_temperature (6, 81, 57)\n",
            "total_precipitation (6, 81, 57)\n",
            "['2m_temperature', 'total_precipitation']\n",
            "2m_temperature (6, 81, 57)\n",
            "total_precipitation (6, 81, 57)\n",
            "['2m_temperature', 'total_precipitation']\n",
            "2m_temperature (6, 81, 57)\n",
            "total_precipitation (6, 81, 57)\n",
            "['2m_temperature', 'total_precipitation']\n",
            "2m_temperature (6, 81, 57)\n",
            "total_precipitation (6, 81, 57)\n",
            "['2m_temperature', 'total_precipitation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DirectForecast(IterableDataset):\n",
        "    def __init__(self, dataset, src, pred_range=6, history=3, window=6):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.history = history\n",
        "        if src == \"era5\":\n",
        "            self.pred_range = pred_range\n",
        "            self.window = window\n",
        "        elif src == \"mpi-esm1-2-hr\":\n",
        "            assert pred_range % 6 == 0\n",
        "            assert window % 6 == 0\n",
        "            self.pred_range = pred_range // 6\n",
        "            self.window = window // 6\n",
        "\n",
        "    def __iter__(self):\n",
        "        for inp_data, out_data, variables, out_variables in self.dataset:\n",
        "            inp_data = {\n",
        "                k: torch.from_numpy(inp_data[k].astype(np.float32))\n",
        "                .unsqueeze(0)\n",
        "                .repeat_interleave(self.history, dim=0)\n",
        "                for k in inp_data.keys()\n",
        "            }\n",
        "            out_data = {\n",
        "                k: torch.from_numpy(out_data[k].astype(np.float32))\n",
        "                for k in out_data.keys()\n",
        "            }\n",
        "            for key in inp_data.keys():\n",
        "                for t in range(self.history):\n",
        "                    inp_data[key][t] = inp_data[key][t].roll(-t * self.window, dims=0)\n",
        "\n",
        "            last_idx = -((self.history - 1) * self.window + self.pred_range)\n",
        "\n",
        "            inp_data = {\n",
        "                k: inp_data[k][:, :last_idx].transpose(0, 1)\n",
        "                for k in inp_data.keys()  # N, T, H, W\n",
        "            }\n",
        "\n",
        "            inp_data_len = inp_data[variables[0]].size(0)\n",
        "\n",
        "            predict_ranges = torch.ones(inp_data_len).to(torch.long) * self.pred_range\n",
        "            output_ids = (\n",
        "                torch.arange(inp_data_len)\n",
        "                + (self.history - 1) * self.window\n",
        "                + predict_ranges\n",
        "            )\n",
        "            out_data = {k: out_data[k][output_ids] for k in out_data.keys()}\n",
        "            yield inp_data, out_data, variables, out_variables"
      ],
      "metadata": {
        "id": "B_TTpnUKGm5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp1 = DirectForecast(tmp, 'era5', pred_range=1, history=2, window=2)"
      ],
      "metadata": {
        "id": "aOec7kZU98Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see what goes in:\n",
        "for xIn, xOut, varIn, varOut in tmp:\n",
        "  print(\"in: \")\n",
        "  for k, v in xIn.items():\n",
        "    print(f'{k.ljust(20)}: {v[:, 0, 0]}')\n",
        "  print(\"out: \")\n",
        "  for k, v in xOut.items():\n",
        "    print(f'{k.ljust(20)}: {v[:, 0, 0]}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTosh_oJ_PDd",
        "outputId": "8cfdfe5d-557b-4967-ed63-435a5045f173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: \n",
            "2m_temperature      : [273.28662 273.08453 271.6197  270.7301  270.22946 270.12387]\n",
            "total_precipitation : [0.23253012 0.7559433  0.7661886  0.7815685  0.85983276 0.98949003]\n",
            "out: \n",
            "2m_temperature      : [273.28662 273.08453 271.6197  270.7301  270.22946 270.12387]\n",
            "total_precipitation : [0.23253012 0.7559433  0.7661886  0.7815685  0.85983276 0.98949003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for xIn, xOut, varIn, varOut in tmp1:\n",
        "  print(\"in: \")\n",
        "  for k, v in xIn.items():\n",
        "    print('-')\n",
        "    print(f'{k.ljust(20)}: {v[:, :, 0, 0]}')\n",
        "  print(\"out: \")\n",
        "  for k, v in xOut.items():\n",
        "    print(f'{k.ljust(20)}: {v[:, 0, 0]}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mVH9AcZ-jgl",
        "outputId": "7d9ba16f-a876-4b83-cd1a-755e8b83d4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: \n",
            "-\n",
            "2m_temperature      : tensor([[273.2866, 271.6197],\n",
            "        [273.0845, 270.7301],\n",
            "        [271.6197, 270.2295]])\n",
            "-\n",
            "total_precipitation : tensor([[0.2325, 0.7662],\n",
            "        [0.7559, 0.7816],\n",
            "        [0.7662, 0.8598]])\n",
            "out: \n",
            "2m_temperature      : tensor([270.7301, 270.2295, 270.1239])\n",
            "total_precipitation : tensor([0.7816, 0.8598, 0.9895])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check input dimensions:\n",
        "for xIn, xOut, varIn, varOut in tmp:\n",
        "  print(\"in: \")\n",
        "  for k, v in xIn.items():\n",
        "    print(f'{k.ljust(20)}: {v.shape}')\n",
        "  print(\"out: \")\n",
        "  for k, v in xOut.items():\n",
        "    print(f'{k.ljust(20)}: {v.shape}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fvGA9IO-qih",
        "outputId": "dca6414b-781c-4f91-85f4-3a26dc69fd1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: \n",
            "2m_temperature      : (6, 81, 57)\n",
            "total_precipitation : (6, 81, 57)\n",
            "out: \n",
            "2m_temperature      : (6, 81, 57)\n",
            "total_precipitation : (6, 81, 57)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the output dim's:\n",
        "for xIn, xOut, varIn, varOut in tmp1:\n",
        "  print(\"in: \")\n",
        "  for k, v in xIn.items():\n",
        "    print(f'{k.ljust(20)}: {v.shape}')\n",
        "  print(\"out: \")\n",
        "  for k, v in xOut.items():\n",
        "    print(f'{k.ljust(20)}: {v.shape}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAgtDEWbDNRO",
        "outputId": "72f4e810-5c91-4d9d-ac3a-210b080b4dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: \n",
            "2m_temperature      : torch.Size([4, 2, 81, 57])\n",
            "total_precipitation : torch.Size([4, 2, 81, 57])\n",
            "out: \n",
            "2m_temperature      : torch.Size([4, 81, 57])\n",
            "total_precipitation : torch.Size([4, 81, 57])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for xIn, xOut, varIn, varOut in tmp1:\n",
        "  print(\"in: \")\n",
        "  for k, v in xIn.items():\n",
        "    print('-')\n",
        "    print(f'{k.ljust(20)}: {v[:, :, 0, 0]}')\n",
        "  print(\"out: \")\n",
        "  for k, v in xOut.items():\n",
        "    print(f'{k.ljust(20)}: {v[:, 0, 0]}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2ZS9hP9DSU2",
        "outputId": "321c819d-0e71-4236-d9b4-d2d830ae2576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: \n",
            "-\n",
            "2m_temperature      : tensor([[273.2866, 273.0845],\n",
            "        [273.0845, 271.6197],\n",
            "        [271.6197, 270.7301],\n",
            "        [270.7301, 270.2295]])\n",
            "-\n",
            "total_precipitation : tensor([[0.2325, 0.7559],\n",
            "        [0.7559, 0.7662],\n",
            "        [0.7662, 0.7816],\n",
            "        [0.7816, 0.8598]])\n",
            "out: \n",
            "2m_temperature      : tensor([271.6197, 270.7301, 270.2295, 270.1239])\n",
            "total_precipitation : tensor([0.7662, 0.7816, 0.8598, 0.9895])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContinuousForecast(IterableDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        random_lead_time=True,\n",
        "        min_pred_range=6,\n",
        "        max_pred_range=120,\n",
        "        hrs_each_step=1,\n",
        "        history=3,\n",
        "        window=6,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not random_lead_time:\n",
        "            assert min_pred_range == max_pred_range\n",
        "        self.dataset = dataset\n",
        "        self.random_lead_time = random_lead_time\n",
        "        self.min_pred_range = min_pred_range\n",
        "        self.max_pred_range = max_pred_range\n",
        "        self.hrs_each_step = hrs_each_step\n",
        "        self.history = history\n",
        "        self.window = window\n",
        "\n",
        "    def __iter__(self):\n",
        "        for inp_data, out_data, variables, out_variables in self.dataset:\n",
        "            inp_data = {\n",
        "                k: torch.from_numpy(inp_data[k].astype(np.float32))\n",
        "                .unsqueeze(0)\n",
        "                .repeat_interleave(self.history, dim=0)\n",
        "                for k in inp_data.keys()\n",
        "            }\n",
        "            out_data = {\n",
        "                k: torch.from_numpy(out_data[k].astype(np.float32))\n",
        "                for k in out_data.keys()\n",
        "            }\n",
        "            for key in inp_data.keys():\n",
        "                for t in range(self.history):\n",
        "                    inp_data[key][t] = inp_data[key][t].roll(-t * self.window, dims=0)\n",
        "\n",
        "            last_idx = -((self.history - 1) * self.window + self.max_pred_range)\n",
        "\n",
        "            inp_data = {\n",
        "                k: inp_data[k][:, :last_idx].transpose(0, 1)\n",
        "                for k in inp_data.keys()  # N, T, H, W\n",
        "            }\n",
        "\n",
        "            inp_data_len = inp_data[variables[0]].size(0)\n",
        "            dtype = inp_data[variables[0]].dtype\n",
        "\n",
        "            if self.random_lead_time:\n",
        "                predict_ranges = torch.randint(\n",
        "                    low=self.min_pred_range,\n",
        "                    high=self.max_pred_range + 1,\n",
        "                    size=(inp_data_len,),\n",
        "                )\n",
        "            else:\n",
        "                predict_ranges = (\n",
        "                    torch.ones(inp_data_len).to(torch.long) * self.max_pred_range\n",
        "                )\n",
        "            lead_times = self.hrs_each_step * predict_ranges / 100\n",
        "            lead_times = lead_times.to(dtype)\n",
        "            output_ids = (\n",
        "                torch.arange(inp_data_len)\n",
        "                + (self.history - 1) * self.window\n",
        "                + predict_ranges\n",
        "            )\n",
        "\n",
        "            out_data = {k: out_data[k][output_ids] for k in out_data.keys()}\n",
        "            yield inp_data, out_data, lead_times, variables, out_variables"
      ],
      "metadata": {
        "id": "fsYxE-VLDzCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp2 = ContinuousForecast(tmp, random_lead_time=True, hrs_each_step=1, min_pred_range=1, max_pred_range=2, history=2, window=2)"
      ],
      "metadata": {
        "id": "6vHisySbE9Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for xIn, xOut, varIn, varOut in tmp:\n",
        "  print(\"in: \")\n",
        "  for k, v in xIn.items():\n",
        "    print('-')\n",
        "    print(f'{k.ljust(20)}: {v[:, 0, 0]}')\n",
        "  print(\"out: \")\n",
        "  for k, v in xOut.items():\n",
        "    print(f'{k.ljust(20)}: {v[:, 0, 0]}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzRJD5eUIxVC",
        "outputId": "3bce220c-6545-45a0-c8f3-59b058a6d3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: \n",
            "-\n",
            "2m_temperature      : [273.28662 273.08453 271.6197  270.7301  270.22946 270.12387]\n",
            "-\n",
            "total_precipitation : [0.23253012 0.7559433  0.7661886  0.7815685  0.85983276 0.98949003]\n",
            "out: \n",
            "2m_temperature      : [273.28662 273.08453 271.6197  270.7301  270.22946 270.12387]\n",
            "total_precipitation : [0.23253012 0.7559433  0.7661886  0.7815685  0.85983276 0.98949003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for xIn, xOut, leadTime, varIn, varOut in tmp2:\n",
        "  print(f\"lead time: {leadTime}\")\n",
        "  print(\"in: \")\n",
        "  for k, v in xIn.items():\n",
        "    print('-')\n",
        "    print(f'{k.ljust(20)}: {v[:, :, 0, 0]}')\n",
        "  print(\"out: \")\n",
        "  for k, v in xOut.items():\n",
        "    print(f'{k.ljust(20)}: {v[:, 0, 0]}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLFM87kVFXlN",
        "outputId": "eb5b3df5-9bef-4214-be6a-a67e501d44f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lead time: tensor([0.0200, 0.0100])\n",
            "in: \n",
            "-\n",
            "2m_temperature      : tensor([[273.2866, 271.6197],\n",
            "        [273.0845, 270.7301]])\n",
            "-\n",
            "total_precipitation : tensor([[0.2325, 0.7662],\n",
            "        [0.7559, 0.7816]])\n",
            "out: \n",
            "2m_temperature      : tensor([270.2295, 270.2295])\n",
            "total_precipitation : tensor([0.8598, 0.8598])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple synthetic test**"
      ],
      "metadata": {
        "id": "M6acnQHsJ6sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeData(IterableDataset):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self._niter = 1\n",
        "        self._nvar = 1\n",
        "        self._vars = [f'k{i}' for i in range(self._nvar)]\n",
        "        self.dim = dim\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self._niter):\n",
        "            yield {k: (i+1)*np.arange(self.dim) for k in self._vars}, {\n",
        "                k: (i+1)*np.arange(self.dim) for k in self._vars\n",
        "            }, self._vars, self._vars"
      ],
      "metadata": {
        "id": "UJcmltsuFnYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printNoLead(x):\n",
        "  for xi, xo, vi, vo in x:\n",
        "    print(\"In: \")\n",
        "    for k, v in xi.items():\n",
        "      print(f'{k.ljust(10)} --> {v}')\n",
        "    print(\"Out: \")\n",
        "    for k, v in xo.items():\n",
        "      print(f'{k.ljust(10)} --> {v}')\n",
        "\n",
        "def printLead(x):\n",
        "  for xi, xo, leadTime, vi, vo in x:\n",
        "    print(\"In: \")\n",
        "    print(f\"lead time: {leadTime*100}\")\n",
        "    for k, v in xi.items():\n",
        "      print(f'{k.ljust(10)} --> {v}')\n",
        "    print(\"Out: \")\n",
        "    for k, v in xo.items():\n",
        "      print(f'{k.ljust(10)} --> {v}')"
      ],
      "metadata": {
        "id": "qeyJexNOPsHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataIn = FakeData(20)"
      ],
      "metadata": {
        "id": "INMB2z8UOrtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printNoLead(dataIn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9rEHcVKQys",
        "outputId": "d4a0581f-fe5d-4a06-fe37-8c94bb0bc56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In: \n",
            "k0         --> [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "Out: \n",
            "k0         --> [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataDirect = DirectForecast(dataIn, 'era5', pred_range=1, history=3, window=5)"
      ],
      "metadata": {
        "id": "etig5t8vLQRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printNoLead(dataDirect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP2WjaHvPolf",
        "outputId": "cbf2845c-4fe6-4a01-b702-ede8bbc902ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In: \n",
            "k0         --> tensor([[ 0.,  5., 10.],\n",
            "        [ 1.,  6., 11.],\n",
            "        [ 2.,  7., 12.],\n",
            "        [ 3.,  8., 13.],\n",
            "        [ 4.,  9., 14.],\n",
            "        [ 5., 10., 15.],\n",
            "        [ 6., 11., 16.],\n",
            "        [ 7., 12., 17.],\n",
            "        [ 8., 13., 18.]])\n",
            "Out: \n",
            "k0         --> tensor([11., 12., 13., 14., 15., 16., 17., 18., 19.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataContinousRand = ContinuousForecast(dataIn, random_lead_time=True, min_pred_range=1, max_pred_range=4,\n",
        "                                   hrs_each_step=1, history=3, window=5)"
      ],
      "metadata": {
        "id": "h92X7K_LP4Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printLead(dataContinousRand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5gK5S7IQzJK",
        "outputId": "5ffc650a-22e6-43c8-b50d-81868ed224ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In: \n",
            "lead time: tensor([1., 2., 1., 1., 3., 1.])\n",
            "k0         --> tensor([[ 0.,  5., 10.],\n",
            "        [ 1.,  6., 11.],\n",
            "        [ 2.,  7., 12.],\n",
            "        [ 3.,  8., 13.],\n",
            "        [ 4.,  9., 14.],\n",
            "        [ 5., 10., 15.]])\n",
            "Out: \n",
            "k0         --> tensor([11., 13., 13., 14., 17., 16.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataContinousFixed = ContinuousForecast(dataIn, random_lead_time=False, min_pred_range=4, max_pred_range=4,\n",
        "                                   hrs_each_step=1, history=4, window=3)"
      ],
      "metadata": {
        "id": "SNJq1GLKQ2Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printLead(dataContinousFixed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc6VvKNOSWJH",
        "outputId": "bce4e253-063b-43ee-beb0-10da4ee335a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In: \n",
            "lead time: tensor([4., 4., 4., 4., 4., 4., 4.])\n",
            "k0         --> tensor([[ 0.,  3.,  6.,  9.],\n",
            "        [ 1.,  4.,  7., 10.],\n",
            "        [ 2.,  5.,  8., 11.],\n",
            "        [ 3.,  6.,  9., 12.],\n",
            "        [ 4.,  7., 10., 13.],\n",
            "        [ 5.,  8., 11., 14.],\n",
            "        [ 6.,  9., 12., 15.]])\n",
            "Out: \n",
            "k0         --> tensor([13., 14., 15., 16., 17., 18., 19.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Downscale(IterableDataset):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __iter__(self):\n",
        "        for inp_data, out_data, variables, out_variables in self.dataset:\n",
        "            inp_data = {\n",
        "                k: torch.from_numpy(inp_data[k].astype(np.float32))\n",
        "                for k in inp_data.keys()\n",
        "            }\n",
        "            out_data = {\n",
        "                k: torch.from_numpy(out_data[k].astype(np.float32))\n",
        "                for k in out_data.keys()\n",
        "            }\n",
        "            yield inp_data, out_data, variables, out_variables"
      ],
      "metadata": {
        "id": "shKEUKjS_r0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IndividualDataIter(IterableDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        transforms,\n",
        "        output_transforms,\n",
        "        subsample=6,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transforms = transforms\n",
        "        self.output_transforms = output_transforms\n",
        "        self.subsample = subsample\n",
        "\n",
        "    def __iter__(self):\n",
        "        for sample in self.dataset:\n",
        "            if isinstance(self.dataset, (DirectForecast, Downscale)):\n",
        "                inp, out, variables, out_variables = sample\n",
        "            elif isinstance(self.dataset, ContinuousForecast):\n",
        "                inp, out, lead_times, variables, out_variables = sample\n",
        "            inp_shapes = set([inp[k].shape[0] for k in inp.keys()])\n",
        "            out_shapes = set([out[k].shape[0] for k in out.keys()])\n",
        "            assert len(inp_shapes) == 1\n",
        "            assert len(out_shapes) == 1\n",
        "            inp_len = next(iter(inp_shapes))\n",
        "            out_len = next(iter(out_shapes))\n",
        "            assert inp_len == out_len\n",
        "            for i in range(0, inp_len, self.subsample):\n",
        "                x = {k: inp[k][i] for k in inp.keys()}\n",
        "                y = {k: out[k][i] for k in out.keys()}\n",
        "                if self.transforms is not None:\n",
        "                    if isinstance(self.dataset, (DirectForecast, ContinuousForecast)):\n",
        "                        x = {\n",
        "                            k: self.transforms[k](x[k].unsqueeze(1)).squeeze(1)\n",
        "                            for k in x.keys()\n",
        "                        }\n",
        "                    elif isinstance(self.dataset, Downscale):\n",
        "                        x = {\n",
        "                            k: self.transforms[k](x[k].unsqueeze(0)).squeeze(0)\n",
        "                            for k in x.keys()\n",
        "                        }\n",
        "                    else:\n",
        "                        raise RuntimeError(f\"Not supported task.\")\n",
        "                if self.output_transforms is not None:\n",
        "                    y = {\n",
        "                        k: self.output_transforms[k](y[k].unsqueeze(0)).squeeze(0)\n",
        "                        for k in y.keys()\n",
        "                    }\n",
        "                if isinstance(self.dataset, (DirectForecast, Downscale)):\n",
        "                    result = x, y, variables, out_variables\n",
        "                elif isinstance(self.dataset, ContinuousForecast):\n",
        "                    result = x, y, lead_times[i], variables, out_variables\n",
        "                yield result"
      ],
      "metadata": {
        "id": "_0DchcIGZ2GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fake data testing individual data iterable"
      ],
      "metadata": {
        "id": "WsIqZVPdAHAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeData(IterableDataset):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self._niter = 1\n",
        "        self._nvar = 2\n",
        "        self._vars = [f'k{i}' for i in range(self._nvar)]\n",
        "        self.dim = dim\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self._niter):\n",
        "            yield {k: (i+1)*np.arange(self.dim) for k in self._vars}, {\n",
        "                k: (i+1)*np.arange(self.dim) for k in self._vars\n",
        "            }, self._vars, self._vars"
      ],
      "metadata": {
        "id": "QDFPLfrkAM7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataIn = FakeData(20)"
      ],
      "metadata": {
        "id": "q2x3qKz-AYIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printNoLead(dataIn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCNbD8qDAbKl",
        "outputId": "0ca27175-df4f-4bab-90f5-4f257757a93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In: \n",
            "k0         --> [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "k1         --> [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "Out: \n",
            "k0         --> [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "k1         --> [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataDirect = DirectForecast(dataIn, 'era5', pred_range=1, history=3, window=5)"
      ],
      "metadata": {
        "id": "jUK6Eo7eAfj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printNoLead(dataDirect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZzV1h7lAk4z",
        "outputId": "377ad1e8-4fc1-447a-baa0-50e97d22a7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In: \n",
            "k0         --> tensor([[ 0.,  5., 10.],\n",
            "        [ 1.,  6., 11.],\n",
            "        [ 2.,  7., 12.],\n",
            "        [ 3.,  8., 13.],\n",
            "        [ 4.,  9., 14.],\n",
            "        [ 5., 10., 15.],\n",
            "        [ 6., 11., 16.],\n",
            "        [ 7., 12., 17.],\n",
            "        [ 8., 13., 18.]])\n",
            "k1         --> tensor([[ 0.,  5., 10.],\n",
            "        [ 1.,  6., 11.],\n",
            "        [ 2.,  7., 12.],\n",
            "        [ 3.,  8., 13.],\n",
            "        [ 4.,  9., 14.],\n",
            "        [ 5., 10., 15.],\n",
            "        [ 6., 11., 16.],\n",
            "        [ 7., 12., 17.],\n",
            "        [ 8., 13., 18.]])\n",
            "Out: \n",
            "k0         --> tensor([11., 12., 13., 14., 15., 16., 17., 18., 19.])\n",
            "k1         --> tensor([11., 12., 13., 14., 15., 16., 17., 18., 19.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directInd = IndividualDataIter(dataDirect, transforms=None, output_transforms=None, subsample=4)"
      ],
      "metadata": {
        "id": "gNtT2MO8-6cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in directInd:\n",
        "  print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlnI0y5J_QfO",
        "outputId": "4a9e5ce5-9400-4568-cd4c-e548b477aae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'k0': tensor([ 0.,  5., 10.]), 'k1': tensor([ 0.,  5., 10.])}, {'k0': tensor(11.), 'k1': tensor(11.)}, ['k0', 'k1'], ['k0', 'k1'])\n",
            "({'k0': tensor([ 4.,  9., 14.]), 'k1': tensor([ 4.,  9., 14.])}, {'k0': tensor(15.), 'k1': tensor(15.)}, ['k0', 'k1'], ['k0', 'k1'])\n",
            "({'k0': tensor([ 8., 13., 18.]), 'k1': tensor([ 8., 13., 18.])}, {'k0': tensor(19.), 'k1': tensor(19.)}, ['k0', 'k1'], ['k0', 'k1'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import Optional, Union"
      ],
      "metadata": {
        "id": "ysCJaottNggZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_crps(\n",
        "    pred: torch.distributions.Normal,\n",
        "    target: Union[torch.FloatTensor, torch.DoubleTensor],\n",
        "    aggregate_only: bool = False,\n",
        "    lat_weights: Optional[Union[torch.FloatTensor, torch.DoubleTensor]] = None,\n",
        ") -> Union[torch.FloatTensor, torch.DoubleTensor]:\n",
        "    mean, std = pred.loc, pred.scale\n",
        "    z = (target - mean) / std\n",
        "    standard_normal = torch.distributions.Normal(\n",
        "        torch.zeros_like(pred), torch.ones_like(pred)\n",
        "    )\n",
        "    pdf = torch.exp(standard_normal.log_prob(z))\n",
        "    cdf = standard_normal.cdf(z)\n",
        "    crps = std * (z * (2 * cdf - 1) + 2 * pdf - 1 / torch.pi)\n",
        "    if lat_weights is not None:\n",
        "        crps = crps * lat_weights\n",
        "    per_channel_losses = crps.mean([0, 2, 3])\n",
        "    loss = crps.mean()\n",
        "    if aggregate_only:\n",
        "        return loss\n",
        "    return torch.cat((per_channel_losses, loss.unsqueeze(0)))"
      ],
      "metadata": {
        "id": "j9KKLbVz5eAk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = (2, 3, 4, 5)"
      ],
      "metadata": {
        "id": "cQg6Lb-q5gtN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.randn(size)\n",
        "s1 = torch.rand(size)"
      ],
      "metadata": {
        "id": "h4YF3l4H5vmp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = torch.distributions.Normal(m1, s1)\n",
        "t1 = m1+0.01"
      ],
      "metadata": {
        "id": "n7InPwwY50De"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "4H7-q0Mt595e",
        "outputId": "ee33bc1a-4461-4206-dd91-2772e84800df"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Normal' object has no attribute 'size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d51a821dc921>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Normal' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p1.loc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckRo4M_G6txl",
        "outputId": "fcefba9b-59a4-4dee-d58d-7cfb23ea4bd4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Metrics classes"
      ],
      "metadata": {
        "id": "9sBnO1sB98Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "from dataclasses import dataclass\n",
        "from functools import wraps\n",
        "from typing import List, Union, Optional\n",
        "\n",
        "# Third party\n",
        "import numpy.typing as npt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "Pred = Union[torch.FloatTensor, torch.DoubleTensor, torch.distributions.Normal]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MetricsMetaInfo:\n",
        "    in_vars: List[str]\n",
        "    out_vars: List[str]\n",
        "    lat: npt.ArrayLike\n",
        "    lon: npt.ArrayLike\n",
        "    climatology: torch.Tensor\n",
        "\n",
        "METRICS_REGISTRY = {}\n",
        "\n",
        "def register(name):\n",
        "    def decorator(metric_class):\n",
        "        METRICS_REGISTRY[name] = metric_class # log\n",
        "        metric_class.name = name # add name attribute to the class\n",
        "        return metric_class\n",
        "\n",
        "    return decorator\n",
        "\n",
        "def handles_probabilistic(metric):\n",
        "    @wraps(metric) # keep the metadata from metric\n",
        "    def wrapper(pred: Pred, *args, **kwargs):\n",
        "        if isinstance(pred, torch.distributions.Normal):\n",
        "            pred = pred.loc\n",
        "        return metric(pred, *args, **kwargs)\n",
        "\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "t6HR9y3c7SR9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric:\n",
        "    \"\"\"Parent class for all ClimateLearn metrics.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, aggregate_only: bool = False, metainfo: Optional[MetricsMetaInfo] = None\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        .. highlight:: python\n",
        "\n",
        "        :param aggregate_only: If false, returns both the aggregate and\n",
        "            per-channel metrics. Otherwise, returns only the aggregate metric.\n",
        "            Default is `False`.\n",
        "        :type aggregate_only: bool\n",
        "        :param metainfo: Optional meta-information used by some metrics.\n",
        "        :type metainfo: MetricsMetaInfo|None\n",
        "        \"\"\"\n",
        "        self.aggregate_only = aggregate_only\n",
        "        self.metainfo = metainfo\n",
        "\n",
        "    def __call__(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param pred: The predicted value(s).\n",
        "        :type pred: torch.Tensor\n",
        "        :param target: The ground truth target value(s).\n",
        "        :type target: torch.Tensor\n",
        "\n",
        "        :return: A tensor. See child classes for specifics.\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class LatitudeWeightedMetric(Metric):\n",
        "    \"\"\"Parent class for latitude-weighted metrics.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, aggregate_only: bool = False, metainfo: Optional[MetricsMetaInfo] = None\n",
        "    ):\n",
        "        super().__init__(aggregate_only, metainfo)\n",
        "        lat_weights = np.cos(np.deg2rad(self.metainfo.lat))\n",
        "        lat_weights = lat_weights / lat_weights.mean()\n",
        "        lat_weights = torch.from_numpy(lat_weights).view(1, 1, -1, 1)\n",
        "        self.lat_weights = lat_weights\n",
        "\n",
        "    def cast_to_device(\n",
        "        self, pred: Union[torch.FloatTensor, torch.DoubleTensor]\n",
        "    ) -> None:\n",
        "        r\"\"\"\n",
        "        .. highlight:: python\n",
        "\n",
        "        Casts latitude weights to the same device as `pred`.\n",
        "        \"\"\"\n",
        "        self.lat_weights = self.lat_weights.to(device=pred.device)"
      ],
      "metadata": {
        "id": "fyII3DRe-v4X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inVar = ['k1', 'k2']\n",
        "outVar = ['k1', 'k2']\n",
        "ddeg=1.\n",
        "lat = np.arange(30.+ddeg/2.,35.,ddeg)\n",
        "lon = np.arange(-120+ddeg/2., -112., ddeg)"
      ],
      "metadata": {
        "id": "S0rkU1OOGGVK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lat.shape, lon.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvEa4mH3HhxC",
        "outputId": "019662ec-3e8a-4940-d2d2-7bcc26d3ce72"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5,) (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minfo = MetricsMetaInfo(inVar, outVar, lat, lon, None)"
      ],
      "metadata": {
        "id": "nQuXArZGHoAl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minfo.lat, minfo.lon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORRgfQFrH20L",
        "outputId": "1d6aa1ea-3be4-409f-ab6a-09d7521ae157"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([30.5, 31.5, 32.5, 33.5, 34.5]),\n",
              " array([-119.5, -118.5, -117.5, -116.5, -115.5, -114.5, -113.5, -112.5]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oneDegGrid =  LatitudeWeightedMetric(aggregate_only=False, metainfo=minfo)"
      ],
      "metadata": {
        "id": "xoI81OhFH95k"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(oneDegGrid.lat_weights)\n",
        "oneDegGrid.lat_weights.shape # B, C, H(lat), W(lon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsNJB8jtLCVr",
        "outputId": "96121d62-1e29-46a0-c8dc-8703471c563d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.0219],\n",
            "          [1.0113],\n",
            "          [1.0003],\n",
            "          [0.9890],\n",
            "          [0.9775]]]], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 5, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate fake data on the same grid"
      ],
      "metadata": {
        "id": "PIyVoZ9GKKBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "size = (batch_size, len(inVar), len(lat), len(lon))\n",
        "pred = torch.ones(size)\n",
        "target = torch.zeros(size)"
      ],
      "metadata": {
        "id": "ExCmsn-bL4C4"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@handles_probabilistic\n",
        "def mse(\n",
        "    pred: Pred,\n",
        "    target: Union[torch.FloatTensor, torch.DoubleTensor],\n",
        "    aggregate_only: bool = False,\n",
        "    lat_weights: Optional[Union[torch.FloatTensor, torch.DoubleTensor]] = None,\n",
        ") -> Union[torch.FloatTensor, torch.DoubleTensor]:\n",
        "    error = (pred - target).square()\n",
        "    if lat_weights is not None:\n",
        "        error = error * lat_weights\n",
        "    per_channel_losses = error.mean([0, 2, 3])\n",
        "    loss = error.mean()\n",
        "    if aggregate_only:\n",
        "        return loss\n",
        "    return torch.cat((per_channel_losses, loss.unsqueeze(0)))\n",
        "\n",
        "@register(\"lat_mse\")\n",
        "class LatWeightedMSE(LatitudeWeightedMetric):\n",
        "    \"\"\"Computes latitude-weighted mean-squared error.\"\"\"\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        pred: Union[torch.FloatTensor, torch.DoubleTensor],\n",
        "        target: Union[torch.FloatTensor, torch.DoubleTensor],\n",
        "    ) -> Union[torch.FloatTensor, torch.DoubleTensor]:\n",
        "        r\"\"\"\n",
        "        .. highlight:: python\n",
        "\n",
        "        :param pred: The predicted values of shape [B,C,H,W].\n",
        "        :type pred: torch.FloatTensor|torch.DoubleTensor\n",
        "        :param target: The ground truth target values of shape [B,C,H,W].\n",
        "        :type target: torch.FloatTensor|torch.DoubleTensor\n",
        "\n",
        "        :return: A singleton tensor if `self.aggregate_only` is `True`. Else, a\n",
        "            tensor of shape [C+1], where the last element is the aggregate\n",
        "            MSE, and the preceding elements are the channel-wise MSEs.\n",
        "        :rtype: torch.FloatTensor|torch.DoubleTensor\n",
        "        \"\"\"\n",
        "        super().cast_to_device(pred)\n",
        "        return mse(pred, target, self.aggregate_only, self.lat_weights)"
      ],
      "metadata": {
        "id": "_HekVxiyJoiv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = LatWeightedMSE(aggregate_only=False, metainfo=minfo)"
      ],
      "metadata": {
        "id": "kh0c9qwFJu6e"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS_REGISTRY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2boI5pM2NOjb",
        "outputId": "39fd7b57-80db-4ffe-dbb9-5bc76578b6a8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lat_mse': __main__.LatWeightedMSE}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp(pred, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-HBSrEoNTjN",
        "outputId": "839a8f53-8a7d-435d-f2ff-8ba78fc84e48"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwW-29faOaaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}